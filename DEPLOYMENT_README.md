# Coworking Analytics Service - Deployment Documentation

This project demonstrates building a containerized Python Flask analytics service, storing its image in Amazon ECR, and deploying to Kubernetes. The stack includes Docker for image packaging, AWS ECR for secure image registry, AWS CodeBuild for CI, and Amazon EKS or any Kubernetes cluster for runtime. CloudWatch Container Insights is used to centralize pod logs and verify health. The Dockerfile uses the official Python slim image, installs dependencies from analytics/requirements.txt, exposes port 5153, configures a health check, and runs as a non-root user. The CodeBuild pipeline logs into ECR, builds the image, tags it with a semantic version, and pushes both the versioned and latest tags. The semantic version defaults to 1.0.0 and can automatically become 1.0.X by deriving the patch from the CodeBuild build number. Deployment is managed with Kubernetes manifests in the deployment directory that create a Deployment, Service, ConfigMap, Secret, and a Postgres Service reference. The coworking Deployment pulls the ECR image, sets environment variables DB_USERNAME, DB_PASSWORD, DB_HOST, DB_PORT, and DB_NAME, and configures readiness and liveness probes. To deploy, update the image URI in deployment.yaml, run kubectl apply -f deployment, and wait for the pod to reach READY. To release a new build, push code to the repo to trigger CodeBuild, confirm the new tag in ECR, update the deployment image tag, and reapply the manifest. Verify the service using kubectl get pods, kubectl get svc, and kubectl describe deployment coworking to ensure a single replica is running. Confirm database networking with kubectl describe svc postgresql and ensure the ConfigMap points to postgresql.default.svc.cluster.local on port 5432. Check CloudWatch Container Insights for the coworking pod to see periodic health log lines without errors. The chosen resource limits are lightweight and tuned for a small Flask API, helping the scheduler place the pod even on modest nodes. Specifically, requests of 100m CPU and 128Mi memory with limits of 250m CPU and 256Mi memory balance stability and cost. These settings leave headroom for occasional spikes while keeping cluster utilization efficient. For instances, a t3.small is adequate for development clusters, while a t3.medium is recommended for production with two vCPUs for resiliency. Burstable CPU credits on T3 keep costs predictable while maintaining responsive builds and deployments. To save costs, prefer spot node groups for non-critical workloads, right-size requests and limits, and enable cluster autoscaling. Also clean up unused ECR images via lifecycle policies and shut down idle environments when not needed.
